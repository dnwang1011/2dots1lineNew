Below is a Cursor-ready refactor checklist that tells you exactly what to edit, why, and how‚Äîfrom schema.prisma all the way to the service layer. Copy each ‚Äú‚úÇÔ∏è  Cursor snippet‚Äù into Cursor and follow the inline TODOs.

‚∏ª

0.  Create a migration branch

git checkout -b feat/memory-refactor



‚∏ª

1.  schema.prisma ‚Äî replace ‚Äúchunks + embeddings‚Äù with one ChunkEmbedding table and add the two pivot tables

‚úÇÔ∏è Cursor snippet  schema.prisma (top of the file)

// -------------- MEMORY v2 --------------
// ChunkEmbedding keeps **both** the text (chunk) and the vector in one row.
// Each chunk can map to many episodes; each episode can map to many thoughts.

model ChunkEmbedding {
  id           String   @id @default(cuid())
  rawDataId    String
  text         String     // full chunk text (<= 4 KB)
  summary      String?    // first-pass LLM TL;DR
  vector       Float[]
  dimension    Int
  importance   Float      // 0-1
  createdAt    DateTime   @default(now())
  userId       String     @db.Uuid
  // pivot
  episodes     ChunkEpisode[] // ‚áÑ many-to-many
  @@index([userId])
}

model Episode {
  id           String    @id @default(cuid())
  title        String
  narrative    String
  centroidVec  Float[]
  centroidDim  Int
  occurredAt   DateTime?
  createdAt    DateTime  @default(now())
  userId       String    @db.Uuid
  // pivots
  chunks       ChunkEpisode[]
  thoughts     EpisodeThought[]
  @@index([userId])
}

model Thought {
  id           String   @id @default(cuid())
  name         String   // short label (‚ÄúMusical Identity Crisis‚Äù)
  description  String   // free-form insight text
  vector       Float[]
  dimension    Int
  createdAt    DateTime @default(now())
  userId       String   @db.Uuid
  episodes     EpisodeThought[]
  @@index([userId])
}

// PIVOT TABLES ‚Äì explicit so that Prisma lets us add extra metadata later
model ChunkEpisode {
  chunkId   String
  episodeId String
  addedAt   DateTime @default(now())
  @@id([chunkId, episodeId])
  chunk     ChunkEmbedding @relation(fields: [chunkId], references: [id], onDelete: Cascade)
  episode   Episode        @relation(fields: [episodeId], references: [id], onDelete: Cascade)
}

model EpisodeThought {
  episodeId String
  thoughtId String
  weight    Float?   // optional ‚Äúrelevance weight‚Äù
  @@id([episodeId, thoughtId])
  episode   Episode  @relation(fields: [episodeId], references: [id], onDelete: Cascade)
  thought   Thought  @relation(fields: [thoughtId], references: [id], onDelete: Cascade)
}

// Keep RawData as-is for provenance

What changed & why

Change	Reason
üî• Deleted SemanticChunk & Embedding	Removes duplication‚Äîone row per chunk, one row per vector (chunk already contains text).
‚ûï ChunkEmbedding.vector	Stores the actual vector so the DB migration can back-fill existing Embedding rows.
‚ûï Two pivot models	Implements many-to-many between chunks ‚Üî episodes and episodes ‚Üî thoughts (user requested).
‚ûï Episode centroid fields	Lets the episodeAgent store the mean vector.
‚ûï Vector on Thought	Enables semantic retrieval on high-level insights.



‚∏ª

2.  Prisma migration

npx prisma migrate dev --name memory_v2

If the existing DB already contains data
	1.	Keep the old tables, run the migration, then execute the back-fill script (step 3).
	2.	When the script finishes, archive / drop Embedding & SemanticChunk.

‚∏ª

3.  Back-fill script (scripts/migrate-memory-v1-v2.ts)

‚úÇÔ∏è Cursor snippet

import { PrismaClient } from '@prisma/client';
const prisma = new PrismaClient();

async function run() {
  const embeddings = await prisma.embedding.findMany({
    include: { chunk: true }
  });

  for (const emb of embeddings) {
    // create or upsert a ChunkEmbedding row
    await prisma.chunkEmbedding.upsert({
      where: { id: emb.chunkId },
      create: {
        id: emb.chunkId,
        rawDataId: emb.rawDataId ?? '',
        text: emb.chunk?.content ?? emb.content,
        summary: emb.summary ?? '',
        vector: emb.vector,
        dimension: emb.dimension,
        importance: emb.importanceScore ?? 0.5,
        userId: emb.perspectiveOwnerId
      },
      update: {}
    });
  }
  console.log(`Migrated ${embeddings.length} vectors ‚ûû ChunkEmbedding`);
}

run().finally(() => prisma.$disconnect());

Run:

ts-node scripts/migrate-memory-v1-v2.ts



‚∏ª

4.  Refactor Memory-layer code

4.1 memoryManager.service.js
	‚Ä¢	Replace every prisma.semanticChunk.create(...) and subsequent prisma.embedding.create(...) with one call:

const chunkRow = await prisma.chunkEmbedding.create({
  data: {
    rawDataId: rawData.id,
    text: chunk.text,
    summary: chunk.summary ?? '',
    vector,
    dimension: vector.length,
    importance: importanceScore,
    userId: rawData.userId
  }
});

	‚Ä¢	On success emit chunk.added with chunkRow.id.

4.2  episodeAgent.js (new file)
	‚Ä¢	Query candidate episodes by user + time window
	‚Ä¢	Compute cosine between chunk.vector and Episode.centroidVec
	‚Ä¢	If ‚â• œµ attach via ChunkEpisode.create
	‚Ä¢	Else push onto orphan queue

4.3  consolidationAgent.js
	‚Ä¢	Load orphans ‚Üí DBSCAN ‚Üí create Episode rows ‚Üí fill ChunkEpisode pivot ‚Üí compute centroid.

4.4  thoughtAgent.js
	‚Ä¢	For each user nightly:
	‚Ä¢	Use Neo4j tags + Weaviate topic clusters to find ‚â• 2 related episodes.
	‚Ä¢	Create Thought, insert into EpisodeThought.

4.5  Retrieval (retrieveMemories)
	‚Ä¢	Stage 1 ‚Äì EpisodeEmbedding search (centroid vectors).
	‚Ä¢	Stage 2 ‚Äì collect linked ChunkEmbedding ids (for raw quotes).
	‚Ä¢	Stage 3 ‚Äì overlay Thought search for high-level summaries.

‚∏ª

5.  Weaviate schema updates
	1.	Delete Memory class.
	2.	Create three flat classes:

Class	Vector	Props
ChunkEmbedding	same as DB	id, text, importance, userId, rawDataId
EpisodeEmbedding	centroidVec	id, title, userId
ThoughtEmbedding	vector	id, name, userId

Update the existing checkWeaviateSchema() helper accordingly.

‚∏ª

6.  Controllers / upload flags
	‚Ä¢	Set forceImportant = true when the MIME type is one of allowedMimeTypes.document so every chunk is kept.
	‚Ä¢	After hefty upload, call orphanQueue.add('upload.boost', { userId }) to trigger consolidation immediately.

‚∏ª

7.  Tests to update

File	Change
tests/memoryManager.spec.ts	expect ChunkEmbedding rows, not SemanticChunk + Embedding.
tests/retrieval.spec.ts	hit /memory/search and validate multi-layer recall.



‚∏ª

8.  Clean-up checklist
	‚Ä¢	Delete old Prisma models (SemanticChunk, Embedding, EmbeddingUpdate).
	‚Ä¢	Remove any chunkId foreign keys in Episode / Thought.
	‚Ä¢	Drop unused columns after verification.
	‚Ä¢	Update GraphQL/OpenAPI schemas if exposed.

‚∏ª

üöÄ  You‚Äôre ready to commit!

git add prisma schema *.service.ts scripts
git commit -m "refactor(memory): merge chunks + embeddings => ChunkEmbedding and add pivots"
git push --set-upstream origin feat/memory-refactor

This brings the backend fully in line with the simplified hybrid-memory design while preserving all functionality‚Äîand prepares the ground for the multi-agent queues we added earlier.